{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "학습방법 : Efficientnet-b7 모델을 사용하고 여러가지 Augmentation을 테스트 하였으며,  Kfold : 3로 나누어 진행하였고 epoch마다 모델을 저장하였습니다. accuracy와 loss를 고려하여 여러가지 모델을 앙상블 진행해보는 것이 목적이였으나, 런타임 중지로 인해 가장 높은 3가지 모델만 추가하여 앙상블을 진행하였습니다. "
      ],
      "metadata": {
        "id": "GYx1p8w9hwHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colab에서 진행."
      ],
      "metadata": {
        "id": "NXCDDOGNqPaX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S76sGVkhxrtE"
      },
      "outputs": [],
      "source": [
        "# Drive 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "코랩 사용 환경 설정"
      ],
      "metadata": {
        "id": "K9ZDcu2ikCBo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lFSzTFd0qGc"
      },
      "outputs": [],
      "source": [
        "# 데이터 unzip, colab 사용 환경 설정\n",
        "from google.colab import output\n",
        "!cp \"/content/drive/MyDrive/MNIST/235697_제 2회 컴퓨터 비전 학습 경진대회_data.zip\" \"data_2.zip\"\n",
        "!unzip \"data_2.zip\"\n",
        "!mkdir \"./dirty_mnist\"\n",
        "!unzip \"dirty_mnist_2nd.zip\" -d \"./dirty_mnist/\"\n",
        "!mkdir \"./test_dirty_mnist\"\n",
        "!unzip \"test_dirty_mnist_2nd.zip\" -d \"./test_dirty_mnist/\"\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 모듈 install\n",
        "!pip install pretrainedmodels\n",
        "!pip install albumentations\n",
        "!pip install --upgrade efficientnet-pytorch"
      ],
      "metadata": {
        "id": "3WSAbNDUkDaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RPgJyf8_QSu"
      },
      "outputs": [],
      "source": [
        "# 필요한 모듈 import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import imutils\n",
        "import zipfile\n",
        "import imgaug\n",
        "import albumentations as A\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import resnet50\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # 디바이스 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "seed, epoch, learning_rate 등 전역 변수 정의 "
      ],
      "metadata": {
        "id": "dxfE1eSdklcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전역 변수 정의\n",
        "class config:\n",
        "  seed = 2022\n",
        "  device = \"cuda\"    \n",
        "  num_classes = 26\n",
        "  lr = 0.001\n",
        "  epochs = 20\n",
        "  train_batch_size = 16\n",
        "  valid_batch_size = 8\n",
        "  num_workers = 3\n",
        "  folds = 3    \n",
        "  IMG_SIZE = 256"
      ],
      "metadata": {
        "id": "L7ts9m1k5Ege"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data 불러오기. 베이스라인 코드를 바탕으로 진행. \n",
        "Augmantation의 경우 Resize, RandomRotate90, RandomContrast 진행. 그밖에  다른 augmantation 다른 세션에서 진행하여 모델 저장."
      ],
      "metadata": {
        "id": "JLJFL-TaoWhg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq_wAhpKBR9E"
      },
      "outputs": [],
      "source": [
        "# Data 불러오기\n",
        "dirty_mnist_answer = pd.read_csv(\"dirty_mnist_2nd_answer.csv\")\n",
        "namelist = os.listdir('./dirty_mnist/')\n",
        "\n",
        "# numpy형식 이미지 Tensor 변환\n",
        "class ToTensor(object):\n",
        "    \"\"\"numpy array를 tensor(torch)로 변환합니다.\"\"\"\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.FloatTensor(image),\n",
        "                'label': torch.FloatTensor(label)}\n",
        "\n",
        "to_tensor = T.Compose([ ToTensor() ])\n",
        "\n",
        "# 이미지 Dataset 변환 클래스\n",
        "class DatasetMNIST(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 dir_path,\n",
        "                 meta_df,\n",
        "                 transforms= to_tensor,\n",
        "                 augmentations = None\n",
        "                 ):\n",
        "        self.dir_path = dir_path\n",
        "        self.meta_df = meta_df    \n",
        "        self.transforms = transforms\n",
        "        self.augmentations = augmentations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.meta_df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = cv2.imread(self.dir_path +\\\n",
        "                           str(self.meta_df.iloc[index,0]).zfill(5) + '.png',\n",
        "                           cv2.IMREAD_GRAYSCALE)\n",
        "        image = cv2.resize(image, dsize=(config.IMG_SIZE, config.IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n",
        "        if self.augmentations:\n",
        "            augmentations = A.Compose([\n",
        "                A.Resize((224,224)),\n",
        "                A.RandomRotation90(p=1),\n",
        "                A.RandomContrast(),\n",
        "            ])\n",
        "            image = augmentations(image=image)['image']\n",
        "        image = (image/255).astype('float')[..., np.newaxis]\n",
        "        label = self.meta_df.iloc[index, 1:].values.astype('float')\n",
        "        sample = {'image': image, 'label': label}\n",
        "        if self.transforms:\n",
        "          sample = self.transforms(sample)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model은 pre-tained된 efficientnet-b7으로 진행. Kfold 3으로 진행하였으나, 런타임 정지로 인해 여러개의 세션들에서 진행된 모델들 앙상블 진행."
      ],
      "metadata": {
        "id": "3MllN693lIF0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf-9aNbagQoV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# 모델 학습\n",
        "kfold = KFold(n_splits=config.folds, shuffle=True, random_state=config.seed)\n",
        "best_models = []\n",
        "\n",
        "# kfold 적용, val_acc가 개선되는 모델 저장\n",
        "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(dirty_mnist_answer),1):\n",
        "    print(f'[fold: {fold_index}]')\n",
        "    torch.cuda.empty_cache()\n",
        "    train_answer = dirty_mnist_answer.iloc[trn_idx]\n",
        "    test_answer  = dirty_mnist_answer.iloc[val_idx]\n",
        "    train_dataset = DatasetMNIST(\"dirty_mnist/\", train_answer)\n",
        "    valid_dataset = DatasetMNIST(\"dirty_mnist/\", test_answer)\n",
        "\n",
        "    train_data_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size = config.train_batch_size,\n",
        "        shuffle = False,\n",
        "        num_workers = config.num_workers\n",
        "    )\n",
        "    valid_data_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size = config.valid_batch_size,\n",
        "        shuffle = False,\n",
        "        num_workers = config.num_workers\n",
        "    )\n",
        "\n",
        "    model = EfficientNet.from_pretrained('efficientnet-b7',in_channels = 1, num_classes= config.num_classes, dropout_rate = 0.3)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.1)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    valid_acc_max = 0\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        train_acc_list = []\n",
        "        with tqdm(train_data_loader,\n",
        "                total=train_data_loader.__len__(),\n",
        "                unit=\"batch\") as train_bar:\n",
        "            for sample in train_bar:\n",
        "                train_bar.set_description(f\"Train Epoch {epoch}\")\n",
        "                optimizer.zero_grad()\n",
        "                images, labels = sample['image'], sample['label']\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                model.train()\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    probs  = model(images)\n",
        "                    loss = criterion(probs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    probs  = probs.cpu().detach().numpy()\n",
        "                    labels = labels.cpu().detach().numpy()\n",
        "                    preds = probs > 0.5\n",
        "                    batch_acc = (labels == preds).mean()\n",
        "                    train_acc_list.append(batch_acc)\n",
        "                    train_acc = np.mean(train_acc_list)\n",
        "\n",
        "                train_bar.set_postfix(train_loss= loss.item(), train_acc = train_acc)\n",
        "                \n",
        "        valid_acc_list = []\n",
        "        with tqdm(valid_data_loader,\n",
        "                total=valid_data_loader.__len__(),\n",
        "                unit=\"batch\") as valid_bar:\n",
        "            for sample in valid_bar:\n",
        "                valid_bar.set_description(f\"Valid Epoch {epoch}\")\n",
        "                optimizer.zero_grad()\n",
        "                images, labels = sample['image'], sample['label']\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    probs  = model(images)\n",
        "                    valid_loss = criterion(probs, labels)\n",
        "\n",
        "                    probs  = probs.cpu().detach().numpy()\n",
        "                    labels = labels.cpu().detach().numpy()\n",
        "                    preds = probs > 0.5\n",
        "                    batch_acc = (labels == preds).mean()\n",
        "                    valid_acc_list.append(batch_acc)\n",
        "\n",
        "                valid_acc = np.mean(valid_acc_list)\n",
        "                valid_bar.set_postfix(valid_loss = valid_loss.item(),\n",
        "                                      valid_acc = valid_acc)\n",
        "            \n",
        "        lr_scheduler.step()\n",
        "\n",
        "        if valid_acc_max < valid_acc:\n",
        "            valid_acc_max = valid_acc\n",
        "            best_model = model\n",
        "            MODEL = \"EfficientnetB7\"\n",
        "            path = \"/content/drive/MyDrive/MNIST/models/\"\n",
        "            torch.save(best_model, f'{path}{fold_index}_{MODEL}_{valid_loss.item():2.4f}_epoch_{epoch}.pth')\n",
        "\n",
        "    best_models.append(best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH0WQqTTieIq"
      },
      "outputs": [],
      "source": [
        "# 학습 결과 확인\n",
        "sample_images = images.cpu().detach().numpy()\n",
        "sample_prob = probs\n",
        "sample_labels = labels\n",
        "\n",
        "idx = 1\n",
        "plt.imshow(sample_images[idx][0])\n",
        "plt.title(\"sample input image\")\n",
        "plt.show()\n",
        "\n",
        "print('예측값 : ',dirty_mnist_answer.columns[1:][sample_prob[idx] > 0.5])\n",
        "print('정답값 : ', dirty_mnist_answer.columns[1:][sample_labels[idx] > 0.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-iJOh15yJQF"
      },
      "outputs": [],
      "source": [
        "# test Dataset 정의\n",
        "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
        "test_dataset = DatasetMNIST(\"test_dirty_mnist/\", sample_submission)\n",
        "batch_size = 16\n",
        "test_data_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = 3,\n",
        "    drop_last = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여러개의 세션에서 진행한 모델중 valid-accuracy와 loss를 고려하여 best_models에 3개의 모델 추가"
      ],
      "metadata": {
        "id": "wfgRNujuovnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 다른 세션에서 파라미터 튜닝을 통해 저장한 model best_models에 불러옴\n",
        "model1 = torch.load('/content/drive/MyDrive/MNIST/models/1_EfficientnetB7_0.5067_epoch_19.pth')\n",
        "model2 = torch.load('/content/drive/MyDrive/MNIST/models/2_EfficientnetB7_0.5641_epoch_19.pth')\n",
        "model3 = torch.load('/content/drive/MyDrive/MNIST/models/1_EfiicientnetB7_0.5919_epoch_12.pth')\n",
        "\n",
        "best_models.append(model1)\n",
        "best_models.append(model2)\n",
        "best_models.append(model3)"
      ],
      "metadata": {
        "id": "gRYwkn1JuU0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "best_models에 있는 model들 앙상블 진행."
      ],
      "metadata": {
        "id": "DDiPz1Gso53I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFTkJEJD0rFQ"
      },
      "outputs": [],
      "source": [
        "# 앙상블 적용\n",
        "predictions_list = []\n",
        "prediction_df = pd.read_csv(\"sample_submission.csv\")\n",
        "\n",
        "for model in best_models:\n",
        "    prediction_array = np.zeros([prediction_df.shape[0],\n",
        "                                 prediction_df.shape[1] -1])\n",
        "    for idx, sample in enumerate(test_data_loader):\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            images = sample['image']\n",
        "            images = images.to(device)\n",
        "            probs  = model(images)\n",
        "            probs = probs.cpu().detach().numpy()\n",
        "            preds = (probs > 0.5)\n",
        "\n",
        "            batch_index = batch_size * idx\n",
        "            prediction_array[batch_index: batch_index + images.shape[0],:]\\\n",
        "                         = preds.astype(int)\n",
        "                         \n",
        "    predictions_list.append(prediction_array[...,np.newaxis])\n",
        "\n",
        "predictions_array = np.concatenate(predictions_list, axis = 2)\n",
        "predictions_mean = predictions_array.mean(axis = 2)\n",
        "predictions_mean = (predictions_mean > 0.5) * 1\n",
        "predictions_mean[1,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMa5pHWC2C1m"
      },
      "outputs": [],
      "source": [
        "# 제출 파일 생성\n",
        "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
        "sample_submission.iloc[:,1:] = predictions_mean\n",
        "sample_submission.to_csv(\"MNIST_prob.csv\", index = False)\n",
        "sample_submission"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}